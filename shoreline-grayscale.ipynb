{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shoreline-grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly let's try to connect to Earth Engine with the python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "\n",
    "def authenticate_and_initialize():\n",
    "    try:\n",
    "        # Authenticate the Earth Engine session.\n",
    "        ee.Authenticate()\n",
    "        # Initialize the Earth Engine module.\n",
    "        ee.Initialize()\n",
    "        print(\"Authentication successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Authentication failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    authenticate_and_initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load images from Earth Engine \n",
    "\n",
    "Users can add their polygon if they want via user input and advanced user could directly modify the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Ask the user if they want to add a new site\n",
    "add_new_site = input(\"Do you want to add a new site? (yes/no): \").strip().lower()\n",
    "\n",
    "if add_new_site == 'yes':\n",
    "    try:\n",
    "        # Ask the user to enter the new site's information\n",
    "        sitename = input(\"Enter the site name (sitename): \")\n",
    "        country = input(\"Enter the country of the site (country): \")\n",
    "\n",
    "        print(\"Enter the polygon coordinates. Type 'done' when you are finished.\")\n",
    "        polygon = []\n",
    "        while True:\n",
    "            coords = input(\"Enter a coordinate in the form 'latitude,longitude': \")\n",
    "            if coords.lower() == 'done':\n",
    "                break\n",
    "            try:\n",
    "                lat, lon = map(float, coords.split(','))\n",
    "                polygon.append([lat, lon])\n",
    "            except ValueError:\n",
    "                print(\"Invalid format. Make sure to enter the coordinates in the form 'latitude,longitude'.\")\n",
    "\n",
    "        # Ensure the polygon is closed by adding the first point to the end\n",
    "        if polygon and polygon[0] != polygon[-1]:\n",
    "            polygon.append(polygon[0])\n",
    "\n",
    "        # Load the existing data from the JSON file\n",
    "        try:\n",
    "            with open('sites_data.json', 'r') as json_file:\n",
    "                data_loaded = json.load(json_file)\n",
    "        except FileNotFoundError:\n",
    "            print(\"The file 'sites_data.json' does not exist. Creating a new file.\")\n",
    "            data_loaded = {\"sites\": []}\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON from the file. Please check the file format.\")\n",
    "            data_loaded = {\"sites\": []}\n",
    "\n",
    "        # Add the new site data to the loaded dictionary\n",
    "        new_site = {\n",
    "            \"sitename\": sitename,\n",
    "            \"country\": country,\n",
    "            \"polygon\": [polygon]\n",
    "        }\n",
    "        data_loaded[\"sites\"].append(new_site)\n",
    "\n",
    "        # Write the updated data back to the JSON file\n",
    "        try:\n",
    "            with open('sites_data.json', 'w') as json_file:\n",
    "                json.dump(data_loaded, json_file, indent=4)\n",
    "            print(\"Data loaded correctly.\")\n",
    "        except IOError as e:\n",
    "            print(f\"An error occurred while writing to the file: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(\"No changes made.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import coastsat functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "plt.ion()\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from coastsat import SDS_download, SDS_preprocess, SDS_shoreline, SDS_tools, SDS_transects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a lots of datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data loaded successfully.\n",
      "Processed polygon for site: NARRA\n",
      "Inputs for site NARRA: {'polygon': [[[151.2957545, -33.7390216], [151.312234, -33.7390216], [151.312234, -33.7012561], [151.2957545, -33.7012561], [151.2957545, -33.7390216]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'NARRA', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Dunkerque\n",
      "Inputs for site Dunkerque: {'polygon': [[[2.22, 51.03], [2.29, 51.03], [2.29, 51.06], [2.22, 51.06], [2.22, 51.03]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Dunkerque', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: DUNKERQUEPORT\n",
      "Inputs for site DUNKERQUEPORT: {'polygon': [[[2.11, 51.01], [2.21, 51.01], [2.21, 51.04], [2.11, 51.04], [2.11, 51.01]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'DUNKERQUEPORT', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: HEMMESDEMARCK\n",
      "Inputs for site HEMMESDEMARCK: {'polygon': [[[1.93, 50.99], [2.09, 50.99], [2.09, 51.03], [1.93, 51.03], [1.93, 50.99]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'HEMMESDEMARCK', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: CALAIS\n",
      "Inputs for site CALAIS: {'polygon': [[[1.79, 50.94], [1.83, 50.94], [1.83, 50.97], [1.79, 50.97], [1.79, 50.94]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'CALAIS', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: HAVRE\n",
      "Inputs for site HAVRE: {'polygon': [[[0.077661, 49.487177], [0.10131, 49.487177], [0.10131, 49.502033], [0.077661, 49.502033], [0.077661, 49.487177]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'HAVRE', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: BoulogneSurMer\n",
      "Inputs for site BoulogneSurMer: {'polygon': [[[1.582218, 50.731713], [1.602387, 50.731713], [1.602387, 50.750249], [1.582218, 50.750249], [1.582218, 50.731713]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'BoulogneSurMer', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Dieppe\n",
      "Inputs for site Dieppe: {'polygon': [[[1.063369, 49.925023], [1.084505, 49.925023], [1.084505, 49.935235], [1.063369, 49.935235], [1.063369, 49.925023]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Dieppe', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Ouistreham\n",
      "Inputs for site Ouistreham: {'polygon': [[[-0.297311, 49.285975], [-0.248999, 49.285975], [-0.248999, 49.30524], [-0.297311, 49.30524], [-0.297311, 49.285975]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Ouistreham', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Cherbourg\n",
      "Inputs for site Cherbourg: {'polygon': [[[-1.586822, 49.651566], [-1.533881, 49.651566], [-1.533881, 49.66006], [-1.586822, 49.66006], [-1.586822, 49.651566]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Cherbourg', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: DIELETTE\n",
      "Inputs for site DIELETTE: {'polygon': [[[-1.864323, 49.550435], [-1.861757, 49.550435], [-1.861757, 49.551485], [-1.864323, 49.551485], [-1.864323, 49.550435]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'DIELETTE', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: SAINTMALO\n",
      "Inputs for site SAINTMALO: {'polygon': [[[-2.023861, 48.651501], [-2.00826, 48.651501], [-2.00826, 48.658002], [-2.023861, 48.658002], [-2.023861, 48.651501]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'SAINTMALO', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: SaintQuayPortrieux\n",
      "Inputs for site SaintQuayPortrieux: {'polygon': [[[-2.82789, 48.64909], [-2.823051, 48.64909], [-2.823051, 48.652827], [-2.82789, 48.652827], [-2.82789, 48.64909]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'SaintQuayPortrieux', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Roscoff\n",
      "Inputs for site Roscoff: {'polygon': [[[-3.974405, 48.710358], [-3.96701, 48.710358], [-3.96701, 48.71471], [-3.974405, 48.71471], [-3.974405, 48.710358]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Roscoff', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: LeConquet\n",
      "Inputs for site LeConquet: {'polygon': [[[-4.7844, 48.3516], [-4.7772, 48.3516], [-4.7772, 48.3588], [-4.7844, 48.3588], [-4.7844, 48.3516]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'LeConquet', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Audierne\n",
      "Inputs for site Audierne: {'polygon': [[[-4.553231, 48.0089], [-4.540745, 48.0089], [-4.540745, 48.012118], [-4.553231, 48.012118], [-4.553231, 48.0089]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Audierne', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Concarneau\n",
      "Inputs for site Concarneau: {'polygon': [[[-3.937105, 47.877159], [-3.927622, 47.877159], [-3.927622, 47.883155], [-3.937105, 47.883155], [-3.937105, 47.877159]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Concarneau', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: LeCrouesty\n",
      "Inputs for site LeCrouesty: {'polygon': [[[-2.897973, 47.533594], [-2.877613, 47.533594], [-2.877613, 47.539207], [-2.897973, 47.539207], [-2.897973, 47.533594]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'LeCrouesty', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: SaintNazaire\n",
      "Inputs for site SaintNazaire: {'polygon': [[[-2.222445, 47.26559], [-2.204948, 47.26559], [-2.204948, 47.272018], [-2.222445, 47.272018], [-2.222445, 47.26559]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'SaintNazaire', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: LHerbaudiere\n",
      "Inputs for site LHerbaudiere: {'polygon': [[[-2.29598, 47.023441], [-2.282448, 47.023441], [-2.282448, 47.029585], [-2.29598, 47.029585], [-2.29598, 47.023441]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'LHerbaudiere', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: AiguillonSurMer\n",
      "Inputs for site AiguillonSurMer: {'polygon': [[[-1.350276, 46.295113], [-1.297692, 46.295113], [-1.297692, 46.341915], [-1.350276, 46.341915], [-1.350276, 46.295113]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'AiguillonSurMer', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Mimizan\n",
      "Inputs for site Mimizan: {'polygon': [[[-1.3063, 44.210877], [-1.289873, 44.210877], [-1.289873, 44.237838], [-1.3063, 44.237838], [-1.3063, 44.210877]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Mimizan', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Lerwick\n",
      "Inputs for site Lerwick: {'polygon': [[[-1.139972, 60.152684], [-1.139206, 60.152684], [-1.139206, 60.153071], [-1.139972, 60.153071], [-1.139972, 60.152684]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Lerwick', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Kinlochbervie\n",
      "Inputs for site Kinlochbervie: {'polygon': [[[-5.058947, 58.459082], [-5.052905, 58.459082], [-5.052905, 58.464222], [-5.058947, 58.464222], [-5.058947, 58.459082]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Kinlochbervie', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Aberdeen\n",
      "Inputs for site Aberdeen: {'polygon': [[[-2.080847, 57.156298], [-2.070614, 57.156298], [-2.070614, 57.165618], [-2.080847, 57.165618], [-2.080847, 57.156298]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Aberdeen', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Wick\n",
      "Inputs for site Wick: {'polygon': [[[-3.084574, 58.432605], [-3.060935, 58.432605], [-3.060935, 58.442819], [-3.084574, 58.442819], [-3.084574, 58.432605]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Wick', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Tobermory\n",
      "Inputs for site Tobermory: {'polygon': [[[-6.06812, 56.62214], [-6.06713, 56.62214], [-6.06713, 56.62318], [-6.06812, 56.62318], [-6.06812, 56.62214]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Tobermory', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Millport\n",
      "Inputs for site Millport: {'polygon': [[[-4.929817, 55.748885], [-4.9131, 55.748885], [-4.9131, 55.75784], [-4.929817, 55.75784], [-4.929817, 55.748885]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Millport', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Leith\n",
      "Inputs for site Leith: {'polygon': [[[-3.18299, 55.98832], [-3.17538, 55.98832], [-3.17538, 55.99176], [-3.18299, 55.99176], [-3.18299, 55.98832]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Leith', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Portrush\n",
      "Inputs for site Portrush: {'polygon': [[[-6.65918, 55.19934], [-6.65424, 55.19934], [-6.65424, 55.20578], [-6.65918, 55.20578], [-6.65918, 55.19934]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Portrush', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Processed polygon for site: Workington\n",
      "Inputs for site Workington: {'polygon': [[[-3.57988, 54.64305], [-3.57488, 54.64305], [-3.57488, 54.65172], [-3.57988, 54.65172], [-3.57988, 54.64305]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Workington', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Updated JSON data saved successfully.\n",
      "Debug: Inputs for site NARRA: {'polygon': [[[151.2957545, -33.7390216], [151.312234, -33.7390216], [151.312234, -33.7012561], [151.2957545, -33.7012561], [151.2957545, -33.7390216]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'NARRA', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: NARRA, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Dunkerque: {'polygon': [[[2.22, 51.03], [2.29, 51.03], [2.29, 51.06], [2.22, 51.06], [2.22, 51.03]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Dunkerque', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Dunkerque, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site DUNKERQUEPORT: {'polygon': [[[2.11, 51.01], [2.21, 51.01], [2.21, 51.04], [2.11, 51.04], [2.11, 51.01]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'DUNKERQUEPORT', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: DUNKERQUEPORT, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site HEMMESDEMARCK: {'polygon': [[[1.93, 50.99], [2.09, 50.99], [2.09, 51.03], [1.93, 51.03], [1.93, 50.99]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'HEMMESDEMARCK', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: HEMMESDEMARCK, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site CALAIS: {'polygon': [[[1.79, 50.94], [1.83, 50.94], [1.83, 50.97], [1.79, 50.97], [1.79, 50.94]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'CALAIS', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: CALAIS, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site HAVRE: {'polygon': [[[0.077661, 49.487177], [0.10131, 49.487177], [0.10131, 49.502033], [0.077661, 49.502033], [0.077661, 49.487177]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'HAVRE', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: HAVRE, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site BoulogneSurMer: {'polygon': [[[1.582218, 50.731713], [1.602387, 50.731713], [1.602387, 50.750249], [1.582218, 50.750249], [1.582218, 50.731713]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'BoulogneSurMer', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: BoulogneSurMer, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Dieppe: {'polygon': [[[1.063369, 49.925023], [1.084505, 49.925023], [1.084505, 49.935235], [1.063369, 49.935235], [1.063369, 49.925023]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Dieppe', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Dieppe, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Ouistreham: {'polygon': [[[-0.297311, 49.285975], [-0.248999, 49.285975], [-0.248999, 49.30524], [-0.297311, 49.30524], [-0.297311, 49.285975]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Ouistreham', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Ouistreham, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Cherbourg: {'polygon': [[[-1.586822, 49.651566], [-1.533881, 49.651566], [-1.533881, 49.66006], [-1.586822, 49.66006], [-1.586822, 49.651566]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Cherbourg', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Cherbourg, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site DIELETTE: {'polygon': [[[-1.864323, 49.550435], [-1.861757, 49.550435], [-1.861757, 49.551485], [-1.864323, 49.551485], [-1.864323, 49.550435]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'DIELETTE', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: DIELETTE, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site SAINTMALO: {'polygon': [[[-2.023861, 48.651501], [-2.00826, 48.651501], [-2.00826, 48.658002], [-2.023861, 48.658002], [-2.023861, 48.651501]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'SAINTMALO', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: SAINTMALO, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site SaintQuayPortrieux: {'polygon': [[[-2.82789, 48.64909], [-2.823051, 48.64909], [-2.823051, 48.652827], [-2.82789, 48.652827], [-2.82789, 48.64909]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'SaintQuayPortrieux', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: SaintQuayPortrieux, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Roscoff: {'polygon': [[[-3.974405, 48.710358], [-3.96701, 48.710358], [-3.96701, 48.71471], [-3.974405, 48.71471], [-3.974405, 48.710358]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Roscoff', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Roscoff, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site LeConquet: {'polygon': [[[-4.7844, 48.3516], [-4.7772, 48.3516], [-4.7772, 48.3588], [-4.7844, 48.3588], [-4.7844, 48.3516]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'LeConquet', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: LeConquet, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Audierne: {'polygon': [[[-4.553231, 48.0089], [-4.540745, 48.0089], [-4.540745, 48.012118], [-4.553231, 48.012118], [-4.553231, 48.0089]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Audierne', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Audierne, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Concarneau: {'polygon': [[[-3.937105, 47.877159], [-3.927622, 47.877159], [-3.927622, 47.883155], [-3.937105, 47.883155], [-3.937105, 47.877159]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Concarneau', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Concarneau, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site LeCrouesty: {'polygon': [[[-2.897973, 47.533594], [-2.877613, 47.533594], [-2.877613, 47.539207], [-2.897973, 47.539207], [-2.897973, 47.533594]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'LeCrouesty', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: LeCrouesty, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site SaintNazaire: {'polygon': [[[-2.222445, 47.26559], [-2.204948, 47.26559], [-2.204948, 47.272018], [-2.222445, 47.272018], [-2.222445, 47.26559]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'SaintNazaire', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: SaintNazaire, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site LHerbaudiere: {'polygon': [[[-2.29598, 47.023441], [-2.282448, 47.023441], [-2.282448, 47.029585], [-2.29598, 47.029585], [-2.29598, 47.023441]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'LHerbaudiere', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: LHerbaudiere, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site AiguillonSurMer: {'polygon': [[[-1.350276, 46.295113], [-1.297692, 46.295113], [-1.297692, 46.341915], [-1.350276, 46.341915], [-1.350276, 46.295113]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'AiguillonSurMer', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: AiguillonSurMer, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Mimizan: {'polygon': [[[-1.3063, 44.210877], [-1.289873, 44.210877], [-1.289873, 44.237838], [-1.3063, 44.237838], [-1.3063, 44.210877]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Mimizan', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Mimizan, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Lerwick: {'polygon': [[[-1.139972, 60.152684], [-1.139206, 60.152684], [-1.139206, 60.153071], [-1.139972, 60.153071], [-1.139972, 60.152684]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Lerwick', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Lerwick, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Kinlochbervie: {'polygon': [[[-5.058947, 58.459082], [-5.052905, 58.459082], [-5.052905, 58.464222], [-5.058947, 58.464222], [-5.058947, 58.459082]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Kinlochbervie', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Kinlochbervie, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Aberdeen: {'polygon': [[[-2.080847, 57.156298], [-2.070614, 57.156298], [-2.070614, 57.165618], [-2.080847, 57.165618], [-2.080847, 57.156298]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Aberdeen', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Aberdeen, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Wick: {'polygon': [[[-3.084574, 58.432605], [-3.060935, 58.432605], [-3.060935, 58.442819], [-3.084574, 58.442819], [-3.084574, 58.432605]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Wick', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Both animations already exist for site: Wick, skipping shoreline mapping and animation creation.\n",
      "Debug: Inputs for site Tobermory: {'polygon': [[[-6.06812, 56.62214], [-6.06713, 56.62214], [-6.06713, 56.62318], [-6.06812, 56.62318], [-6.06812, 56.62214]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Tobermory', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Number of images available between 1984-01-01 and 2025-01-01:\n",
      "- In Landsat Tier 1 & Sentinel-2 Level-1C:\n",
      "     L5: 722 images\n",
      "     L7: 564 images\n",
      "     L8: 541 images\n",
      "     L9: 126 images\n",
      "  Total to download: 1953 images\n",
      "L5: 722 images already exist, 0 to download\n",
      "L7: 564 images already exist, 0 to download\n",
      "L8: 541 images already exist, 519 to download\n",
      "- In Landsat Tier 2 (not suitable for time-series analysis):\n",
      "     L5: 341 images\n",
      "     L7: 75 images\n",
      "     L8: 85 images\n",
      "  Total Tier 2: 501 images\n",
      "Checked image availability for site: Tobermory\n",
      "Number of images available between 1984-01-01 and 2025-01-01:\n",
      "- In Landsat Tier 1 & Sentinel-2 Level-1C:\n",
      "     L5: 722 images\n",
      "     L7: 564 images\n",
      "     L8: 541 images\n",
      "     L9: 126 images\n",
      "  Total to download: 1953 images\n",
      "L5: 722 images already exist, 0 to download\n",
      "L7: 564 images already exist, 0 to download\n",
      "L8: 541 images already exist, 519 to download\n",
      "- In Landsat Tier 2 (not suitable for time-series analysis):\n",
      "     L5: 341 images\n",
      "     L7: 75 images\n",
      "     L8: 85 images\n",
      "  Total Tier 2: 501 images\n",
      "\n",
      "Downloading images:\n",
      "L5: 0 images\n",
      "\n",
      "L7: 0 images\n",
      "\n",
      "L8: 519 images\n",
      "4%\n",
      "Download failed, trying again...\n",
      "18%\n",
      "Download failed, trying again...\n",
      "100%\n",
      "L9: 126 images\n",
      "100%\n",
      "Satellite images downloaded from GEE and save in /home/athena/Document/Shoreline-Grayscale/data/Tobermory\n",
      "Saving images as jpg:\n",
      "L5: 722 images\n",
      "722/722\n",
      "L7: 564 images\n",
      "564/564\n",
      "L8: 541 images\n",
      "541/541\n",
      "L9: 126 images\n",
      "126/126\n",
      "Satellite images saved as .jpg in /home/athena/Document/Shoreline-Grayscale/data/Tobermory/jpg_files/preprocessed\n",
      "Images saved for site: Tobermory\n",
      "Getting reference shoreline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MESA: error: ZINK: failed to choose pdev\n",
      "glx: failed to create drisw screen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference shoreline has been saved in /home/athena/Document/Shoreline-Grayscale/data/Tobermory\n",
      "Extracting shorelines...\n",
      "Mapping shorelines:\n",
      "L5:   0%Automatic mode activated\n",
      "L5:   27%Skipped image '1991-10-03-10-51-52' due to high cloud cover: 0.58\n",
      "L5:   45%Skipped image '1996-05-09-10-39-02' due to high cloud cover: 0.58\n",
      "L5:   71%Skipped image '2003-07-16-11-04-41' due to high cloud cover: 0.75\n",
      "L5:   100%\n",
      "L7:   28%Skipped image '2007-04-30-11-18-44' due to high cloud cover: 0.76\n",
      "L7:   31%Skipped image '2011-08-15-11-21-17' due to high cloud cover: 0.74\n",
      "L7:   31%Skipped image '2011-08-15-11-21-41' due to high cloud cover: 0.74\n",
      "L7:   33%Skipped image '2012-06-14-11-22-17' due to high cloud cover: 0.58\n",
      "L7:   34%Skipped image '2012-09-11-11-17-30' due to high cloud cover: 0.69\n",
      "L7:   35%Skipped image '2012-10-13-11-17-48' due to high cloud cover: 0.51\n",
      "L7:   36%Skipped image '2013-04-14-11-23-53' due to high cloud cover: 0.68\n",
      "L7:   39%Skipped image '2013-09-14-11-17-44' due to high cloud cover: 0.82\n",
      "L7:   42%Skipped image '2014-05-12-11-19-23' due to high cloud cover: 0.96\n",
      "L7:   43%Skipped image '2014-06-29-11-19-35' due to high cloud cover: 0.51\n",
      "L7:   43%Skipped image '2014-07-15-11-19-36' due to high cloud cover: 0.78\n",
      "L7:   44%Skipped image '2014-07-31-11-19-44' due to high cloud cover: 0.96\n",
      "L7:   45%Skipped image '2014-11-04-11-20-18' due to high cloud cover: 0.87\n",
      "L7:   46%Skipped image '2015-03-28-11-21-09' due to high cloud cover: 0.96\n",
      "L7:   48%Skipped image '2015-07-09-11-27-59' due to high cloud cover: 0.66\n",
      "L7:   49%Skipped image '2015-07-25-11-27-38' due to high cloud cover: 0.66\n",
      "L7:   49%Skipped image '2015-07-25-11-28-02' due to high cloud cover: 0.66\n",
      "L7:   52%Skipped image '2016-02-11-11-23-58' due to high cloud cover: 0.51\n",
      "L7:   54%Skipped image '2016-04-22-11-30-12' due to high cloud cover: 0.62\n",
      "L7:   54%Skipped image '2016-04-22-11-30-36' due to high cloud cover: 0.62\n",
      "L7:   58%Skipped image '2016-10-08-11-24-51' due to high cloud cover: 0.78\n",
      "L7:   60%Skipped image '2017-04-02-11-24-21' due to high cloud cover: 0.96\n",
      "L7:   63%Skipped image '2017-08-24-11-24-43' due to high cloud cover: 0.64\n",
      "L7:   64%Skipped image '2017-09-25-11-24-41' due to high cloud cover: 0.60\n",
      "L7:   64%Skipped image '2017-10-11-11-24-43' due to high cloud cover: 0.82\n",
      "L7:   65%Skipped image '2018-01-31-11-23-48' due to high cloud cover: 0.82\n",
      "L7:   69%Skipped image '2018-06-08-11-21-55' due to high cloud cover: 0.78\n",
      "L7:   70%Skipped image '2018-07-10-11-21-18' due to high cloud cover: 0.60\n",
      "L7:   70%Skipped image '2018-08-11-11-20-44' due to high cloud cover: 0.51\n",
      "L7:   73%Skipped image '2019-02-26-11-21-22' due to high cloud cover: 0.80\n",
      "L7:   73%Skipped image '2019-02-26-11-21-46' due to high cloud cover: 0.80\n",
      "L7:   75%Skipped image '2019-04-24-11-13-38' due to high cloud cover: 0.82\n",
      "L7:   78%Skipped image '2019-10-17-11-06-36' due to high cloud cover: 0.60\n",
      "L7:   81%Skipped image '2020-06-04-11-00-10' due to high cloud cover: 0.62\n",
      "L7:   84%Skipped image '2021-02-24-10-36-37' due to high cloud cover: 0.78\n",
      "L7:   84%Skipped image '2021-03-19-10-40-35' due to high cloud cover: 0.68\n",
      "L7:   85%Skipped image '2021-03-19-10-40-59' due to high cloud cover: 0.64\n",
      "L7:   85%Skipped image '2021-04-20-10-37-57' due to high cloud cover: 0.68\n",
      "L7:   85%Skipped image '2021-04-20-10-38-21' due to high cloud cover: 0.68\n",
      "L7:   85%Skipped image '2021-05-06-10-36-36' due to high cloud cover: 0.88\n",
      "L7:   86%Skipped image '2021-05-06-10-37-00' due to high cloud cover: 0.88\n",
      "L7:   88%Skipped image '2021-08-19-10-21-53' due to high cloud cover: 0.69\n",
      "L7:   88%Skipped image '2021-08-26-10-27-04' due to high cloud cover: 0.68\n",
      "L7:   88%Skipped image '2021-08-26-10-27-28' due to high cloud cover: 0.68\n",
      "L7:   96%Skipped image '2023-04-18-09-24-09' due to high cloud cover: 0.51\n",
      "L7:   99%Skipped image '2023-09-02-09-06-09' due to high cloud cover: 0.82\n",
      "L7:   100%Skipped image '2023-10-09-09-09-22' due to high cloud cover: 0.68\n",
      "\n",
      "L8:   100%\n",
      "L9:   100%\n",
      "Removing duplicates...\n",
      "466 duplicates\n",
      "59 bad georef\n",
      "Creating animation MP4 of shorelines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rawvideo @ 0x5e22700] Stream #0: not enough frames to estimate rate; consider increasing probesize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animation has been generated (using 4 frames per second) and saved at /home/athena/Document/Shoreline-Grayscale/data/Tobermory/Tobermory_animation_shorelines.mp4\n",
      "Shoreline animation created for site: Tobermory\n",
      "Shoreline plot saved for site: Tobermory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rawvideo @ 0x6d2f700] Stream #0: not enough frames to estimate rate; consider increasing probesize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animation has been generated (using 4 frames per second) and saved at /home/athena/Document/Shoreline-Grayscale/data/Tobermory/Tobermory_animation_RGB.mp4\n",
      "RGB animation created for site: Tobermory\n",
      "Starting shoreline analysis...\n",
      "Shoreline output loaded successfully.\n",
      "466 duplicates\n",
      "59 bad georef\n",
      "Shoreline analysis completed.\n",
      "Debug: Inputs for site Millport: {'polygon': [[[-4.929817, 55.748885], [-4.9131, 55.748885], [-4.9131, 55.75784], [-4.929817, 55.75784], [-4.929817, 55.748885]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Millport', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Number of images available between 1984-01-01 and 2025-01-01:\n",
      "- In Landsat Tier 1 & Sentinel-2 Level-1C:\n",
      "     L5: 483 images\n",
      "     L7: 388 images\n",
      "     L8: 348 images\n",
      "     L9: 90 images\n",
      "  Total to download: 1309 images\n",
      "- In Landsat Tier 2 (not suitable for time-series analysis):\n",
      "     L5: 270 images\n",
      "     L7: 52 images\n",
      "     L8: 65 images\n",
      "  Total Tier 2: 387 images\n",
      "Checked image availability for site: Millport\n",
      "Number of images available between 1984-01-01 and 2025-01-01:\n",
      "- In Landsat Tier 1 & Sentinel-2 Level-1C:\n",
      "     L5: 483 images\n",
      "     L7: 388 images\n",
      "     L8: 348 images\n",
      "     L9: 90 images\n",
      "  Total to download: 1309 images\n",
      "- In Landsat Tier 2 (not suitable for time-series analysis):\n",
      "     L5: 270 images\n",
      "     L7: 52 images\n",
      "     L8: 65 images\n",
      "  Total Tier 2: 387 images\n",
      "\n",
      "Downloading images:\n",
      "L5: 483 images\n",
      "0%\n",
      "Download failed, trying again...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define date range\n",
    "dates = ['1984-01-01', '2025-01-01']\n",
    "\n",
    "fps = 4\n",
    "\n",
    "# Define satellite missions\n",
    "sat_list = ['L5', 'L7', 'L8', 'L9']\n",
    "\n",
    "# Choose Landsat collection\n",
    "collection = 'C02'\n",
    "\n",
    "# Directory where the data will be stored\n",
    "filepath = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "settings = {\n",
    "    # General parameters:\n",
    "    'cloud_thresh': 0.5,        # Threshold on maximum cloud cover\n",
    "    'dist_clouds': 300,         # Distance around clouds where shoreline can't be mapped\n",
    "    'output_epsg': 28356,       # EPSG code of spatial reference system desired for the output\n",
    "    # Quality control:\n",
    "    'check_detection': True,    # If True, shows each shoreline detection to the user for validation\n",
    "    'adjust_detection': False,  # If True, allows user to adjust the position of each shoreline by changing the threshold\n",
    "    'save_figure': True,        # If True, saves a figure showing the mapped shoreline for each image\n",
    "    # [ONLY FOR ADVANCED USERS] Shoreline detection parameters:\n",
    "    'min_beach_area': 1000,     # Minimum area (in metres^2) for an object to be labelled as a beach\n",
    "    'min_length_sl': 500,       # Minimum length (in metres) of shoreline perimeter to be valid\n",
    "    'cloud_mask_issue': False,  # Switch this parameter to True if sand pixels are masked (in black) on many images\n",
    "    'sand_color': 'default',    # 'default', 'latest', 'dark' (for grey/black sand beaches) or 'bright' (for white sand beaches)\n",
    "    'pan_off': False,           # True to switch pansharpening off for Landsat 7/8/9 imagery\n",
    "    's2cloudless_prob': 60,     # Probability threshold to identify cloudy pixels in the s2cloudless mask\n",
    "}\n",
    "\n",
    "# Load the JSON file\n",
    "try:\n",
    "    with open('json/sites_data.json', 'r') as json_file:\n",
    "        data_loaded = json.load(json_file)\n",
    "        print(\"JSON data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"The file 'sites_data.json' does not exist.\")\n",
    "    data_loaded = {\"sites\": []}\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error decoding JSON from the file. Please check the file format.\")\n",
    "    data_loaded = {\"sites\": []}\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    data_loaded = {\"sites\": []}\n",
    "\n",
    "# Process each site and add inputs to each site dictionary\n",
    "for site in data_loaded[\"sites\"]:\n",
    "    try:\n",
    "        sitename = site['sitename']\n",
    "        country = site.get('country', 'Unknown')  # Default to 'Unknown' if not present\n",
    "        polygon = site['polygon']\n",
    "\n",
    "        # Convert the polygon to the smallest rectangle (sides parallel to coordinate axes)\n",
    "        polygon = SDS_tools.smallest_rectangle(polygon)\n",
    "        print(f\"Processed polygon for site: {sitename}\")\n",
    "\n",
    "        # Put all the inputs into a dictionary\n",
    "        inputs = {\n",
    "            'polygon': polygon,\n",
    "            'dates': dates,\n",
    "            'sat_list': sat_list,\n",
    "            'sitename': sitename,\n",
    "            'filepath': filepath,\n",
    "            'landsat_collection': collection\n",
    "        }\n",
    "\n",
    "        # Add the inputs dictionary to the site dictionary\n",
    "        site['inputs'] = inputs\n",
    "        print(f\"Inputs for site {sitename}: {inputs}\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Key error: {e} for site: {site}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing site: {sitename}. Error: {e}\")\n",
    "\n",
    "# Save the updated JSON file with inputs added\n",
    "try:\n",
    "    with open('json/sites_data_updated.json', 'w') as json_file:\n",
    "        json.dump(data_loaded, json_file, indent=4)\n",
    "    print(\"Updated JSON data saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the updated JSON data: {e}\")\n",
    "\n",
    "# Use the updated JSON file for further processing\n",
    "for site in data_loaded[\"sites\"]:\n",
    "    try:\n",
    "        sitename = site['sitename']\n",
    "        inputs = site['inputs']\n",
    "\n",
    "        # Debug: print inputs to trace errors\n",
    "        print(f\"Debug: Inputs for site {sitename}: {inputs}\")\n",
    "\n",
    "        # Merge settings with inputs for this site\n",
    "        site_settings = settings.copy()\n",
    "        site_settings.update(inputs)  # Directly update settings with inputs\n",
    "        site_settings['inputs'] = inputs  # Also add inputs as a sub-dictionary in settings\n",
    "\n",
    "        # Check if shoreline animation already exists\n",
    "        fn_animation_shorelines = os.path.join(inputs['filepath'], inputs['sitename'], f'{inputs[\"sitename\"]}_animation_shorelines.mp4')\n",
    "        fn_animation_rgb = os.path.join(inputs['filepath'], inputs['sitename'], f'{inputs[\"sitename\"]}_animation_RGB.mp4')\n",
    "        \n",
    "        if os.path.exists(fn_animation_shorelines) and os.path.exists(fn_animation_rgb):\n",
    "            print(f\"Both animations already exist for site: {sitename}, skipping shoreline mapping and animation creation.\")\n",
    "            continue\n",
    "\n",
    "        # Before downloading the images, check how many images are available for your inputs\n",
    "        SDS_download.check_images_available(site_settings)\n",
    "        print(f\"Checked image availability for site: {sitename}\")\n",
    "        metadata = SDS_download.retrieve_images(site_settings)\n",
    "        metadata2 = SDS_download.get_metadata(site_settings)\n",
    "\n",
    "        # Check if images already exist\n",
    "        image_path = os.path.join(inputs['filepath'], inputs['sitename'], 'jpg_files', 'preprocessed')\n",
    "        if not os.path.exists(image_path):\n",
    "            os.makedirs(image_path)\n",
    "        \n",
    "        if not any(os.scandir(image_path)):  # If directory is empty\n",
    "            # Save the images\n",
    "            SDS_preprocess.save_jpg(metadata, site_settings, use_matplotlib=True)\n",
    "            print(f\"Images saved for site: {sitename}\")\n",
    "        else:\n",
    "            print(f\"Images already exist for site: {sitename}, skipping save_jpg.\")\n",
    "\n",
    "        # Process shoreline extraction and reference shoreline if shorelines animation does not exist\n",
    "        if not os.path.exists(fn_animation_shorelines):\n",
    "            print(\"Getting reference shoreline...\")\n",
    "            settings['reference_shoreline'] = SDS_preprocess.get_reference_sl(metadata, site_settings)\n",
    "            settings['max_dist_ref'] = 100  # max distance (in meters) allowed from the reference shoreline\n",
    "            print(\"Extracting shorelines...\")\n",
    "            \n",
    "            output = SDS_shoreline.extract_shorelines(metadata, site_settings)\n",
    "            print(\"Removing duplicates...\")\n",
    "            output = SDS_tools.remove_duplicates(output)  # removes duplicates (images taken on the same date by the same satellite)\n",
    "            output = SDS_tools.remove_inaccurate_georef(output, 10)  # remove inaccurate georeferencing (set threshold to 10 m)\n",
    "\n",
    "            # Create MP4 timelapse animation of shorelines\n",
    "            fp_images = os.path.join(inputs['filepath'], inputs['sitename'], 'jpg_files', 'detection')\n",
    "            fps = 4  # frames per second in animation\n",
    "            print(\"Creating animation MP4 of shorelines...\")\n",
    "            SDS_tools.make_animation_mp4(fp_images, fps, fn_animation_shorelines)\n",
    "            print(f\"Shoreline animation created for site: {sitename}\")\n",
    "\n",
    "            # Plot and save the shoreline data\n",
    "            fig = plt.figure(figsize=[15, 8])\n",
    "            plt.axis('equal')\n",
    "            plt.xlabel('Eastings')\n",
    "            plt.ylabel('Northings')\n",
    "            plt.grid(linestyle=':', color='0.5')\n",
    "            for i in range(len(output['shorelines'])):\n",
    "                sl = output['shorelines'][i]\n",
    "                date = output['dates'][i]\n",
    "                plt.plot(sl[:, 0], sl[:, 1], '.', label=date.strftime('%d-%m-%Y'))\n",
    "            plt.legend()\n",
    "            plt.title(f'Shorelines for site: {sitename}')\n",
    "            plt.savefig(os.path.join(inputs['filepath'], inputs['sitename'], 'shoreline_all_time.jpg'))\n",
    "            plt.close()\n",
    "            print(f\"Shoreline plot saved for site: {sitename}\")\n",
    "\n",
    "        # Create MP4 timelapse animation of RGB images if it does not exist\n",
    "        if not os.path.exists(fn_animation_rgb):\n",
    "            fp_images_rgb = os.path.join(inputs['filepath'], inputs['sitename'], 'jpg_files', 'preprocessed')\n",
    "            SDS_tools.make_animation_mp4(fp_images_rgb, fps, fn_animation_rgb)\n",
    "            print(f\"RGB animation created for site: {sitename}\")\n",
    "        else:\n",
    "            print(f\"RGB animation already exists for site: {sitename}\")\n",
    "\n",
    "        # Shoreline analysis: Compute time-series of cross-shore distance along user-defined shore-normal transects\n",
    "        print(\"Starting shoreline analysis...\")\n",
    "        filepath = os.path.join(inputs['filepath'], sitename)\n",
    "        try:\n",
    "            with open(os.path.join(filepath, sitename + '_output.pkl'), 'rb') as f:\n",
    "                output = pickle.load(f)\n",
    "                print(\"Shoreline output loaded successfully.\")\n",
    "            # Remove duplicates and inaccurate georeferencing\n",
    "            output = SDS_tools.remove_duplicates(output)\n",
    "            output = SDS_tools.remove_inaccurate_georef(output, 10)\n",
    "            print(\"Shoreline analysis completed.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Output file for site {sitename} not found. Skipping analysis.\")\n",
    "        except pickle.PickleError:\n",
    "            print(f\"Error loading pickle file for site {sitename}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during shoreline analysis for site {sitename}: {e}\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Key error: {e} for site: {site}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing site: {sitename}. Error: {e}\")\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transects implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data loaded successfully.\n",
      "Debug: Inputs for site NARRA: {'polygon': [[[151.2957545, -33.7390216], [151.312234, -33.7390216], [151.312234, -33.7012561], [151.2957545, -33.7012561], [151.2957545, -33.7390216]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'NARRA', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Pickle file loaded successfully.\n",
      "Pickle file loaded successfully.\n",
      "Transects file already exists for site: NARRA. Skipping transect drawing.\n",
      "Time-series plotted for site: NARRA\n",
      "CSV saved to '/home/athena/Document/Shoreline-Grayscale/data/NARRA/transect_time_series.csv' successfully.\n",
      "Debug: Inputs for site Dunkerque: {'polygon': [[[2.22, 51.03], [2.29, 51.03], [2.29, 51.06], [2.22, 51.06], [2.22, 51.03]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'Dunkerque', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Pickle file loaded successfully.\n",
      "Drawing transects interactively for site: Dunkerque\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MESA: error: ZINK: failed to choose pdev\n",
      "glx: failed to create drisw screen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred while processing transects for site: Dunkerque. Error: list index out of range\n",
      "Debug: Inputs for site DUNKERQUEPORT: {'polygon': [[[2.11, 51.01], [2.21, 51.01], [2.21, 51.04], [2.11, 51.04], [2.11, 51.01]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'DUNKERQUEPORT', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Pickle file loaded successfully.\n",
      "Pickle file loaded successfully.\n",
      "Transects file already exists for site: DUNKERQUEPORT. Skipping transect drawing.\n",
      "Time-series plotted for site: DUNKERQUEPORT\n",
      "CSV saved to '/home/athena/Document/Shoreline-Grayscale/data/DUNKERQUEPORT/transect_time_series.csv' successfully.\n",
      "Debug: Inputs for site HEMMESDEMARCK: {'polygon': [[[1.93, 50.99], [2.09, 50.99], [2.09, 51.03], [1.93, 51.03], [1.93, 50.99]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'HEMMESDEMARCK', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Pickle file loaded successfully.\n",
      "Drawing transects interactively for site: HEMMESDEMARCK\n",
      "Transect locations saved in /home/athena/Document/Shoreline-Grayscale/data/HEMMESDEMARCK\n",
      "Data saved to '/home/athena/Document/Shoreline-Grayscale/data/HEMMESDEMARCK/HEMMESDEMARCK_transects.pkl' successfully.\n",
      "Time-series plotted for site: HEMMESDEMARCK\n",
      "CSV saved to '/home/athena/Document/Shoreline-Grayscale/data/HEMMESDEMARCK/transect_time_series.csv' successfully.\n",
      "Debug: Inputs for site CALAIS: {'polygon': [[[1.79, 50.94], [1.83, 50.94], [1.83, 50.97], [1.79, 50.97], [1.79, 50.94]]], 'dates': ['1984-01-01', '2025-01-01'], 'sat_list': ['L5', 'L7', 'L8', 'L9'], 'sitename': 'CALAIS', 'filepath': '/home/athena/Document/Shoreline-Grayscale/data', 'landsat_collection': 'C02'}\n",
      "Pickle file loaded successfully.\n",
      "Drawing transects interactively for site: CALAIS\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 174\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDrawing transects interactively for site: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msitename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 174\u001b[0m     transects \u001b[38;5;241m=\u001b[39m \u001b[43mSDS_transects\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_transects\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     save_pickle(transects, transects_file)\n\u001b[1;32m    177\u001b[0m settings_transects \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malong_dist\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m25\u001b[39m}\n",
      "File \u001b[0;32m~/Document/Shoreline-Grayscale/coastsat/SDS_transects.py:123\u001b[0m, in \u001b[0;36mdraw_transects\u001b[0;34m(output, settings)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# loop until user breaks it by click <enter>\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# let user click two points\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     pts \u001b[38;5;241m=\u001b[39m \u001b[43mginput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    125\u001b[0m         origin \u001b[38;5;241m=\u001b[39m pts[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/coastsat/lib/python3.12/site-packages/matplotlib/pyplot.py:2761\u001b[0m, in \u001b[0;36mginput\u001b[0;34m(n, timeout, show_clicks, mouse_add, mouse_pop, mouse_stop)\u001b[0m\n\u001b[1;32m   2752\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39mginput)\n\u001b[1;32m   2753\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mginput\u001b[39m(\n\u001b[1;32m   2754\u001b[0m     n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2759\u001b[0m     mouse_stop: MouseButton \u001b[38;5;241m=\u001b[39m MouseButton\u001b[38;5;241m.\u001b[39mMIDDLE,\n\u001b[1;32m   2760\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m-> 2761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgcf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mginput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_clicks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_clicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmouse_add\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmouse_add\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmouse_pop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmouse_pop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmouse_stop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmouse_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2768\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/coastsat/lib/python3.12/site-packages/matplotlib/figure.py:3483\u001b[0m, in \u001b[0;36mFigure.ginput\u001b[0;34m(self, n, timeout, show_clicks, mouse_add, mouse_pop, mouse_stop)\u001b[0m\n\u001b[1;32m   3480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(clicks) \u001b[38;5;241m==\u001b[39m n \u001b[38;5;129;01mand\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3481\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mstop_event_loop()\n\u001b[0;32m-> 3483\u001b[0m \u001b[43m_blocking_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocking_input_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbutton_press_event\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey_press_event\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[38;5;66;03m# Cleanup.\u001b[39;00m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mark \u001b[38;5;129;01min\u001b[39;00m marks:\n",
      "File \u001b[0;32m~/miniconda3/envs/coastsat/lib/python3.12/site-packages/matplotlib/_blocking_input.py:26\u001b[0m, in \u001b[0;36mblocking_input_loop\u001b[0;34m(figure, event_names, timeout, handler)\u001b[0m\n\u001b[1;32m     24\u001b[0m cids \u001b[38;5;241m=\u001b[39m [figure\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mmpl_connect(name, handler) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m event_names]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Start event loop.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:  \u001b[38;5;66;03m# Run even on exception like ctrl-c.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Disconnect the callbacks.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cid \u001b[38;5;129;01min\u001b[39;00m cids:\n",
      "File \u001b[0;32m~/miniconda3/envs/coastsat/lib/python3.12/site-packages/matplotlib/backends/backend_qt.py:451\u001b[0m, in \u001b[0;36mFigureCanvasQT.start_event_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    449\u001b[0m     _ \u001b[38;5;241m=\u001b[39m QtCore\u001b[38;5;241m.\u001b[39mQTimer\u001b[38;5;241m.\u001b[39msingleShot(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m), event_loop\u001b[38;5;241m.\u001b[39mquit)\n\u001b[0;32m--> 451\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_allow_interrupt_qt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_loop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqt_compat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_loop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/coastsat/lib/python3.12/contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/coastsat/lib/python3.12/site-packages/matplotlib/backend_bases.py:1672\u001b[0m, in \u001b[0;36m_allow_interrupt\u001b[0;34m(prepare_notifier, handle_sigint)\u001b[0m\n\u001b[1;32m   1670\u001b[0m signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGINT, old_sigint_handler)\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handler_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1672\u001b[0m     \u001b[43mold_sigint_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhandler_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QSocketNotifier: Invalid socket 87 and type 'Read', disabling...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use Agg backend for headless environments\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import pandas as pd\n",
    "from coastsat import SDS_transects\n",
    "\n",
    "def load_json(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            print(\"JSON data loaded successfully.\")\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{filepath}' does not exist.\")\n",
    "        return {\"sites\": []}\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON from the file. Please check the file format.\")\n",
    "        return {\"sites\": []}\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return {\"sites\": []}\n",
    "\n",
    "def load_pickle(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            print(\"Pickle file loaded successfully.\")\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{filepath}' not found.\")\n",
    "        return None\n",
    "    except pickle.PickleError:\n",
    "        print(f\"Error loading pickle file '{filepath}'.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading pickle file '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "def save_pickle(data, filepath):\n",
    "    try:\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "            print(f\"Data saved to '{filepath}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while saving to '{filepath}': {e}\")\n",
    "\n",
    "def save_csv(data, filepath):\n",
    "    try:\n",
    "        data.to_csv(filepath, sep=',')\n",
    "        print(f\"CSV saved to '{filepath}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while saving CSV to '{filepath}': {e}\")\n",
    "\n",
    "def compute_intersection_QC(output, transects, settings):\n",
    "    cross_dist = dict([])\n",
    "    shorelines = output['shorelines']\n",
    "    along_dist = settings['along_dist']\n",
    "\n",
    "    for key in transects.keys():\n",
    "        std_intersect = np.zeros(len(shorelines))\n",
    "        med_intersect = np.zeros(len(shorelines))\n",
    "        max_intersect = np.zeros(len(shorelines))\n",
    "        min_intersect = np.zeros(len(shorelines))\n",
    "        n_intersect = np.zeros(len(shorelines))\n",
    "        \n",
    "        for i in range(len(shorelines)):\n",
    "            sl = shorelines[i]\n",
    "            X0 = transects[key][0,0]\n",
    "            Y0 = transects[key][0,1]\n",
    "            temp = np.array(transects[key][-1,:]) - np.array(transects[key][0,:])\n",
    "            phi = np.arctan2(temp[1], temp[0])\n",
    "            Mrot = np.array([[np.cos(phi), np.sin(phi)],[-np.sin(phi), np.cos(phi)]])\n",
    "            p1 = np.array([X0,Y0])\n",
    "            p2 = transects[key][-1,:]\n",
    "            d_line = np.abs(np.cross(p2-p1,sl-p1)/np.linalg.norm(p2-p1))\n",
    "            d_origin = np.array([np.linalg.norm(sl[k,:] - p1) for k in range(len(sl))])\n",
    "            idx_dist = np.logical_and(d_line <= along_dist, d_origin <= 1000)\n",
    "            idx_close = np.where(idx_dist)[0]\n",
    "\n",
    "            if len(idx_close) == 0:\n",
    "                std_intersect[i] = np.nan\n",
    "                med_intersect[i] = np.nan\n",
    "                max_intersect[i] = np.nan\n",
    "                min_intersect[i] = np.nan\n",
    "                n_intersect[i] = np.nan\n",
    "            else:\n",
    "                xy_close = np.array([sl[idx_close,0],sl[idx_close,1]]) - np.tile(np.array([[X0],[Y0]]), (1,len(sl[idx_close])))\n",
    "                xy_rot = np.matmul(Mrot, xy_close)\n",
    "                xy_rot[0, xy_rot[0,:] < settings['min_chainage']] = np.nan\n",
    "\n",
    "                if not np.isnan(xy_rot[0,:]).all():\n",
    "                    std_intersect[i] = np.nanstd(xy_rot[0,:])\n",
    "                    med_intersect[i] = np.nanmedian(xy_rot[0,:])\n",
    "                    max_intersect[i] = np.nanmax(xy_rot[0,:])\n",
    "                    min_intersect[i] = np.nanmin(xy_rot[0,:])\n",
    "                    n_intersect[i] = np.sum(~np.isnan(xy_rot[0, :]))\n",
    "                else:\n",
    "                    std_intersect[i] = np.nan\n",
    "                    med_intersect[i] = np.nan\n",
    "                    max_intersect[i] = np.nan\n",
    "                    min_intersect[i] = np.nan\n",
    "                    n_intersect[i] = np.nan\n",
    "\n",
    "        condition1 = std_intersect <= settings['max_std']\n",
    "        condition2 = (max_intersect - min_intersect) <= settings['max_range']\n",
    "        condition3 = n_intersect >= settings['min_points']\n",
    "        idx_good = np.logical_and(np.logical_and(condition1, condition2), condition3)\n",
    "\n",
    "        if settings['multiple_inter'] == 'auto':\n",
    "            prc_over = np.sum(std_intersect > settings['max_std'])/len(std_intersect)\n",
    "            if prc_over > settings['auto_prc']:\n",
    "                med_intersect[~idx_good] = max_intersect[~idx_good]\n",
    "                med_intersect[~condition3] = np.nan\n",
    "            else:\n",
    "                med_intersect[~idx_good] = np.nan\n",
    "        elif settings['multiple_inter'] == 'max':\n",
    "            med_intersect[~idx_good] = max_intersect[~idx_good]\n",
    "            med_intersect[~condition3] = np.nan\n",
    "            prc_over = 0\n",
    "        elif settings['multiple_inter'] == 'nan':\n",
    "            med_intersect[~idx_good] = np.nan\n",
    "            prc_over = 0\n",
    "        else:\n",
    "            raise Exception('the multiple_inter parameter can only be: nan, max or auto')\n",
    "\n",
    "        cross_dist[key] = med_intersect\n",
    "\n",
    "    return cross_dist\n",
    "\n",
    "# Load the updated JSON file with inputs added\n",
    "data_loaded = load_json('json/sites_data_updated.json')\n",
    "\n",
    "# Process each site to define transects and generate time-series\n",
    "for site in data_loaded[\"sites\"]:\n",
    "    try:\n",
    "        sitename = site['sitename']\n",
    "        inputs = site['inputs']\n",
    "        print(f\"Debug: Inputs for site {sitename}: {inputs}\")\n",
    "\n",
    "        filepath = os.path.join(inputs['filepath'], sitename)\n",
    "        output = load_pickle(os.path.join(filepath, sitename + '_output.pkl'))\n",
    "        if output is None:\n",
    "            print(f\"Skipping transect processing for site {sitename} due to missing output file.\")\n",
    "            continue\n",
    "\n",
    "        transects_file = os.path.join(filepath, sitename + '_transects.pkl')\n",
    "        if os.path.exists(transects_file):\n",
    "            transects = load_pickle(transects_file)\n",
    "            print(f\"Transects file already exists for site: {sitename}. Skipping transect drawing.\")\n",
    "        else:\n",
    "            settings = {\n",
    "                'cloud_thresh': 0.5,\n",
    "                'dist_clouds': 300,\n",
    "                'output_epsg': 28356,\n",
    "                'check_detection': True,\n",
    "                'adjust_detection': False,\n",
    "                'save_figure': True,\n",
    "                'min_beach_area': 1000,\n",
    "                'min_length_sl': 500,\n",
    "                'cloud_mask_issue': False,\n",
    "                'sand_color': 'default',\n",
    "                'pan_off': False,\n",
    "                's2cloudless_prob': 60,\n",
    "                'inputs': inputs,\n",
    "            }\n",
    "\n",
    "            print(f\"Drawing transects interactively for site: {sitename}\")\n",
    "            %matplotlib qt\n",
    "            transects = SDS_transects.draw_transects(output, settings)\n",
    "            save_pickle(transects, transects_file)\n",
    "\n",
    "        settings_transects = {'along_dist': 25}\n",
    "        cross_distance = SDS_transects.compute_intersection(output, transects, settings_transects)\n",
    "\n",
    "        settings_transects_qc = {\n",
    "            'along_dist': 25,\n",
    "            'min_points': 3,\n",
    "            'max_std': 15,\n",
    "            'max_range': 30,\n",
    "            'min_chainage': -100,\n",
    "            'multiple_inter': 'auto',\n",
    "            'auto_prc': 0.1,\n",
    "        }\n",
    "        cross_distance_qc = compute_intersection_QC(output, transects, settings_transects_qc)\n",
    "\n",
    "        fig = plt.figure(figsize=[15, 8], tight_layout=True)\n",
    "        gs = gridspec.GridSpec(len(cross_distance_qc), 1)\n",
    "        gs.update(left=0.05, right=0.95, bottom=0.05, top=0.95, hspace=0.05)\n",
    "        for i, key in enumerate(cross_distance_qc.keys()):\n",
    "            if np.all(np.isnan(cross_distance_qc[key])):\n",
    "                continue\n",
    "            ax = fig.add_subplot(gs[i, 0])\n",
    "            ax.grid(linestyle=':', color='0.5')\n",
    "            ax.plot(output['dates'], cross_distance_qc[key], '-o', ms=6, mfc='w')\n",
    "            ax.set_ylabel('distance [m]', fontsize=12)\n",
    "            ax.text(0.5, 0.95, key, bbox=dict(boxstyle=\"square\", ec='k', fc='w'), ha='center',\n",
    "                    va='top', transform=ax.transAxes, fontsize=14)\n",
    "        plt.savefig(os.path.join(filepath, 'transects_time_series.jpg'))\n",
    "        plt.close()\n",
    "        print(f\"Time-series plotted for site: {sitename}\")\n",
    "\n",
    "        out_dict = dict([])\n",
    "        out_dict['dates'] = output['dates']\n",
    "        for key in transects.keys():\n",
    "            out_dict[key] = cross_distance_qc[key]\n",
    "        df = pd.DataFrame(out_dict)\n",
    "        fn = os.path.join(filepath, 'transect_time_series.csv')\n",
    "        save_csv(df, fn)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Key error: {e} for site: {site}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing transects for site: {sitename}. Error: {e}\")\n",
    "\n",
    "print(\"Transect processing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from coastsat import SDS_download, SDS_preprocess, SDS_shoreline, SDS_tools, SDS_transects\n",
    "\n",
    "# Load the updated JSON file with inputs added\n",
    "try:\n",
    "    with open('json/sites_data_updated.json', 'r') as json_file:\n",
    "        data_loaded = json.load(json_file)\n",
    "        print(\"JSON data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"The file 'sites_data_updated.json' does not exist.\")\n",
    "    data_loaded = {\"sites\": []}\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error decoding JSON from the file. Please check the file format.\")\n",
    "    data_loaded = {\"sites\": []}\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    data_loaded = {\"sites\": []}\n",
    "\n",
    "# Process each site to define transects and generate time-series\n",
    "for site in data_loaded[\"sites\"]:\n",
    "    try:\n",
    "        sitename = site['sitename']\n",
    "        inputs = site['inputs']\n",
    "\n",
    "        # Debug: print inputs to trace errors\n",
    "        print(f\"Debug: Inputs for site {sitename}: {inputs}\")\n",
    "\n",
    "        # Load shoreline output file\n",
    "        filepath = os.path.join(inputs['filepath'], sitename)\n",
    "        try:\n",
    "            with open(os.path.join(filepath, sitename + '_output.pkl'), 'rb') as f:\n",
    "                output = pickle.load(f)\n",
    "                print(\"Shoreline output loaded successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Output file for site {sitename} not found. Skipping transect processing.\")\n",
    "            continue\n",
    "        except pickle.PickleError:\n",
    "            print(f\"Error loading pickle file for site {sitename}.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during shoreline loading for site {sitename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Check if transects file exists\n",
    "        transects_file = os.path.join(filepath, sitename + '_transects.pkl')\n",
    "        if os.path.exists(transects_file):\n",
    "            with open(transects_file, 'rb') as f:\n",
    "                transects = pickle.load(f)\n",
    "            print(f\"Transects file already exists for site: {sitename}. Skipping transect drawing.\")\n",
    "        else:\n",
    "            # Combine settings with inputs\n",
    "            settings = {\n",
    "                'cloud_thresh': 0.5,\n",
    "                'dist_clouds': 300,\n",
    "                'output_epsg': 28356,\n",
    "                'check_detection': True,\n",
    "                'adjust_detection': False,\n",
    "                'save_figure': True,\n",
    "                'min_beach_area': 1000,\n",
    "                'min_length_sl': 500,\n",
    "                'cloud_mask_issue': False,\n",
    "                'sand_color': 'default',\n",
    "                'pan_off': False,\n",
    "                's2cloudless_prob': 60,\n",
    "                'inputs': inputs,\n",
    "            }\n",
    "\n",
    "            # Draw shore-normal transects interactively\n",
    "            print(f\"Drawing transects interactively for site: {sitename}\")\n",
    "            %matplotlib qt\n",
    "            transects = SDS_transects.draw_transects(output, settings)\n",
    "            \n",
    "            # Save transects to a file\n",
    "            with open(transects_file, 'wb') as f:\n",
    "                pickle.dump(transects, f)\n",
    "            print(f\"Transects saved for site: {sitename}\")\n",
    "\n",
    "        # Intersect shorelines with transects to obtain time-series of cross-shore distance\n",
    "        settings_transects = {'along_dist': 25}\n",
    "        cross_distance = SDS_transects.compute_intersection(output, transects, settings_transects)\n",
    "\n",
    "        # Quality control intersections\n",
    "        settings_transects_qc = {\n",
    "            'along_dist': 25,\n",
    "            'min_points': 3,\n",
    "            'max_std': 15,\n",
    "            'max_range': 30,\n",
    "            'min_chainage': -100,\n",
    "            'multiple_inter': 'auto',\n",
    "            'auto_prc': 0.1,\n",
    "        }\n",
    "        cross_distance_qc = SDS_transects.compute_intersection_QC(output, transects, settings_transects_qc)\n",
    "\n",
    "        # Despiking the time-series\n",
    "        settings_outliers = {'max_cross_change': 40, 'otsu_threshold': [-.5, 0], 'plot_fig': True}\n",
    "        cross_distance_qc = SDS_transects.reject_outliers(cross_distance_qc, output, settings_outliers)\n",
    "\n",
    "        # Seasonal averaging and long-term trends\n",
    "        season_colors = {'DJF':'C3', 'MAM':'C1', 'JJA':'C2', 'SON':'C0'}\n",
    "        for key in cross_distance_qc.keys():\n",
    "            chainage = cross_distance_qc[key]\n",
    "            idx_nan = np.isnan(chainage)\n",
    "            dates_nonan = [output['dates'][i] for i in np.where(~idx_nan)[0]]\n",
    "            chainage = chainage[~idx_nan] \n",
    "            \n",
    "            dict_seas, dates_seas, chainage_seas, list_seas = SDS_transects.seasonal_average(dates_nonan, chainage)\n",
    "            trend, y = SDS_transects.calculate_trend(dates_seas, chainage_seas)\n",
    "            \n",
    "            fig, ax = plt.subplots(1, 1, figsize=[14, 4], tight_layout=True)\n",
    "            ax.grid(which='major', linestyle=':', color='0.5')\n",
    "            ax.set_title(f'Time-series at {key}', x=0, ha='left')\n",
    "            ax.set(ylabel='distance [m]')\n",
    "            ax.plot(dates_nonan, chainage, '+', lw=1, color='k', mfc='w', ms=4, alpha=0.5, label='raw datapoints')\n",
    "            ax.plot(dates_seas, chainage_seas, '-', lw=1, color='k', mfc='w', ms=4, label='seasonally-averaged')\n",
    "            for seas in dict_seas.keys():\n",
    "                ax.plot(dict_seas[seas]['dates'], dict_seas[seas]['chainages'], 'o', mec='k', color=season_colors[seas], label=seas, ms=5)\n",
    "            ax.plot(dates_seas, y, '--', color='b', label=f'trend {trend:.1f} m/year')\n",
    "            ax.legend(loc='lower left', ncol=7, markerscale=1.5, frameon=True, edgecolor='k', columnspacing=1)\n",
    "            fig.savefig(os.path.join(filepath, f'seasonal_averages_{key}.jpg'))\n",
    "\n",
    "        # Monthly averages\n",
    "        month_colors = plt.get_cmap('tab20')\n",
    "        for key in cross_distance_qc.keys():\n",
    "            chainage = cross_distance_qc[key]\n",
    "            idx_nan = np.isnan(chainage)\n",
    "            dates_nonan = [output['dates'][i] for i in np.where(~idx_nan)[0]]\n",
    "            chainage = chainage[~idx_nan] \n",
    "            \n",
    "            dict_month, dates_month, chainage_month, list_month = SDS_transects.monthly_average(dates_nonan, chainage)\n",
    "            \n",
    "            fig, ax = plt.subplots(1, 1, figsize=[14, 4], tight_layout=True)\n",
    "            ax.grid(which='major', linestyle=':', color='0.5')\n",
    "            ax.set_title(f'Time-series at {key}', x=0, ha='left')\n",
    "            ax.set(ylabel='distance [m]')\n",
    "            ax.plot(dates_nonan, chainage, '+', lw=1, color='k', mfc='w', ms=4, alpha=0.5, label='raw datapoints')\n",
    "            ax.plot(dates_month, chainage_month, '-', lw=1, color='k', mfc='w', ms=4, label='monthly-averaged')\n",
    "            for month in dict_month.keys():\n",
    "                ax.plot(dict_month[month]['dates'], dict_month[month]['chainages'], 'o', mec='k', color=month_colors(month), label=month, ms=5)\n",
    "            ax.legend(loc='lower left', ncol=7, markerscale=1.5, frameon=True, edgecolor='k', columnspacing=1)\n",
    "            fig.savefig(os.path.join(filepath, f'monthly_averages_{key}.jpg'))\n",
    "\n",
    "        # Save the time-series as CSV\n",
    "        out_dict = dict([])\n",
    "        out_dict['dates'] = output['dates']\n",
    "        for key in transects.keys():\n",
    "            out_dict[key] = cross_distance_qc[key]\n",
    "        df = pd.DataFrame(out_dict)\n",
    "        fn = os.path.join(filepath, 'transect_time_series.csv')\n",
    "        df.to_csv(fn, sep=',')\n",
    "        print(f\"Time-series of the shoreline change along the transects saved as: {fn}\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Key error: {e} for site: {site}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing transects for site: {sitename}. Error: {e}\")\n",
    "\n",
    "print(\"Transect processing completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
